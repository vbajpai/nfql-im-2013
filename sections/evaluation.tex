We used the first $20M$ records from the publically available Trace 7 from the
SimpleWeb \cite{simpleweb} repository for our performance evaluations. The
input trace was compressed at \texttt{ZLIB\_LEVEL} $5$ using the \texttt{zlib}
suite. It was also converted to \texttt{nfdump} and SiLK compatible formats
keeping the same compression level. The suite was run on a machine with $24$
cores of $2.5$ GHz clock speed and $18$ GiB of memory.

The first set of queries stress the filter stage.  We use varying values on
the \texttt{packet} field offset to control the amount of flow records that
are passed by the filter. The resultant filtered records are written as
\texttt{flow-tools} files and compressed at \texttt{ZLIB\_LEVEL} $5$. The
ratio of the number of filtered records in the output trace to the number of
the flow records in the input trace is plotted against time. The results are
shown in Fig. \ref{fig:benchmarks-filter}.

\begin{figure}[h!]
  \begin{center}
    \includegraphics* [width=0.9\linewidth]{filter}
    \caption{Filter Stage: NFQL vs SiLK, Flow-Tools, Nfdump}
    \label{fig:benchmarks-filter}
  \end{center}
\end{figure}

It can be seen that the performance of the filter stage in \texttt{nfql} is
comparable to that of \texttt{flowtools} and SiLK. SiLK takes less time on
lower ratios, but then SilK and \texttt{nfdump} also use their own file
format. \texttt{nfdump} appears to be significantly faster than the rest. This
is because \texttt{nfdump} uses the \texttt{lzo} compression scheme. Adding
\texttt{lzo} compression will likely improve \texttt{nfql}'s filter
performance. Note that all the tools were single-threaded
in this evaluation, and did not completely utilize the $24$ available cores. 

The second set of queries stress the grouper stage. We reuse the filter query
that produces a $1.0$ ratio to allow the grouper to receive the entire trace
as a filtered recordset. The grouper part of the query then gradually
increases the number of grouping terms in the \ac{DNF} expression to increase
the output/input ratio. The resultant groups are again written as
\texttt{flowtools} files using the same \texttt{zlib} compression level. The
ratio of the number of groups formed to the number of the input filtered
records is plotted against time. \texttt{nfdump} and \texttt{flowtools} do not
support grouping, and therefore are not considered in this evaluation. The
results are shown in Fig. \ref{fig:benchmarks-grouper}.

\begin{figure}[ht!]
  \begin{center}
    \includegraphics* [width=0.9\linewidth]{grouper}
    \caption{Grouper Stage: NFQL vs SiLK}
    \label{fig:benchmarks-grouper}
  \end{center}
\end{figure}

The evaluation graph reveals that the time taken by the tools are comparable
on lower ratios, but on higher ratios, \texttt{nfql} starts to drift apart.
Since most of the time is taken in writing the records to files, it is unclear
whether SiLK's usage of its own file format is responsible for the drift.
SiLK's \texttt{rwgroup} tool is also supplied a \texttt{-{}-summarize} flag to
force it to write only the first record of each group, to make both tools
write the same number of records. This gives SiLK the leverage to not store
information about which members are part of the group. \texttt{nfql} on the
other hand needs to allocate resources (which may take time) to keep this
information in its data structures, since the ungrouper later may request to
write the members of a group while unfolding the tuples.  It is also important
to note that both the tools again remained single-threaded throughout the
evaluation. SiLK took advantage of an inherent concurency arising from a pipe
between \texttt{rwsort} and \texttt{rwgroup}, which makes the two processes
run concurrently, the effect of which gets more pronounced on higher ratios.
The profiling results from GNU \texttt{gprof} \cite{graham:1982} indicate that
$60\%$ of the time is taken in \texttt{qsort} comparator calls.  As a result,
it comes as no surprise, that bifurcating \texttt{qsort} invocation to
multiple threads and later merging the results back using merge sort will help
parallelize the grouper stage and maybe reduce the drift on higher ratios. In
addition, since all of the evaluation queries had grouping terms using an
equality comparator, \texttt{nfql} can introspect such a grouping rule to
dynamically optimize processing searches using a hashtable and turn to
\texttt{qsort} based grouping only as a fallback.

The third set of queries stress the group filter stage. We reuse
the filter and grouper queries that produce a $1.0$ ratio to allow the group
filter to receive the entire trace as input. This means, each flow record of
the original trace now becomes a group record for the group filter.  The group
filter then reuses the same varying values of the \texttt{packet} field offset
to control the amount of groups that are filtered ahead. The filtered groups
are again written as \texttt{flowtools} files using the same \texttt{zlib}
compression level. The ratio of the number of output filtered groups to the
number of the input group is plotted against time. The results are shown in
Fig. \ref{fig:benchmarks-groupfilter}.

\begin{figure}[ht!]
  \begin{center}
    \includegraphics* [width=0.9\linewidth]{groupfilter}
    \caption{Group Filter Stage: NFQL vs SiLK}
    \label{fig:benchmarks-groupfilter}
  \end{center}
\end{figure}

It can be seen that the timings of \texttt{nfql} are far apart from that of
SiLK.  It is due to the drift already created by the grouper at the $1.0$
ratio in the previous stage. As a result, the group filter comes into play
only after $300$ seconds, whereas SiLK's group filtering already starts just
below $150$ seconds. Even if we normalize the graph, it can be observed that
the \texttt{nfql} group filter has a higher slope. This is because it is only
executed once the grouper returns, and therefore has to reiterate the groups
to make a filtering decision.

The fourth set of queries stress the merger stage. We reuse the filter,
grouper and group filter queries that produce a $1.0$ ratio. These queries are
then run in two separate branches to produce identical filtered group records.
The merger then applies match rules to produce different output to input
ratios. The groups that are merged are again written as \texttt{flowtools}
files using the same \texttt{zlib} compression level. The ratio of the number
of merged groups to twice\footnote{Each branch pushes the entire trace as an
input to the merger.} the number of flow records in the original trace is
plotted against time. A data point for SiLK for the $0.2$ ratio is not
available since the \ac{NFQL} query executed at that data point uses
\texttt{OR} expressions, which are not supported by SiLK.

\begin{figure}[ht!]
  \begin{center}
    \includegraphics* [width=0.9\linewidth]{merger}
    \caption{Merger Stage: NFQL vs SiLK}
    \label{fig:benchmarks-merger}
  \end{center}
\end{figure}

It can be seen that the merger is the most performance critical stage of the
\ac{NFQL} pipeline thus far. It is due to the fact that the merger is working
on twice the number of flow records than any other previous stage. In
addition, each branch is writing the results of the filter, grouper and group
filter stage to \texttt{flowtools} files. As a result, the amount of disk I/O
involved is twice as much as well. Even though each branch is delegated to a
separate core using affinity masks, most of time is taken in writing these
results to the file. These results although look less promising, they are way
better than the original \texttt{nfql} merger implementation. The optimized
merger implementation takes advantage of sorted nature of filtered groups and
therefore can significantly reduce the number of merger matches. It also
writes a merged group record to a file only once despite the number of times
it has matched. Without these optimizations, running such queries on the
merger kept the CPU churning for days without results.

The last set of queries stress the ungrouper stage. They reuse the entire
merger queries as is, but enable the ungrouper as well. This means, that the
ungrouper now attempts to unfold the merged groups returned by the merger to
write their member flow records to \texttt{flowtools} files.  However, since
the merger receives each flow record as its own filtered group, each merged
group has only one member. As a result, the ungrouper ends up rewriting the
merged groups as \texttt{flowtools} files using the same \texttt{zlib}
compression level. This means that the execution engine end up taking twice
the amount of time than the merger. It is important to note, that SiLK does
not have such an equivalent ungrouping tool and is therefore not considered in
this final evaluation.
